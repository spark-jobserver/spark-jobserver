FROM openjdk:8-jdk

ARG spark_version="3.0.2"
ENV SPARK_VERSION=${spark_version}

USER root

# Python version shoule be < 3.8 as it is the last Python version, which works with Spark 2.4
ARG PYTHON_VERSION=3.6.0

RUN apt-get -qq update && apt-get install -y --force-yes libssl-dev openssl build-essential zlib1g-dev && \
    wget https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz && \
    tar xzvf Python-${PYTHON_VERSION}.tgz && \
    cd Python-${PYTHON_VERSION} && \
    ./configure && make && make install && \
    ln -s ./python /usr/bin/python

# install and cache sbt, python
RUN echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | tee -a /etc/apt/sources.list.d/sbt.list && \
    curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | apt-key add && \
    apt-get -qq update && \
    apt-get install -y --force-yes python3 python3-pip sbt=1.5.2 && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /usr/src/app/

# install other ci deps
COPY ci ci
RUN chmod +x ci/*.sh && \
    mv ci/run_tests.sh . && \
    ci/install-python-dependencies.sh && \
    ci/install-spark.sh

# add sbt and cache deps
COPY project project
COPY build.sbt .
RUN sbt update

# add the rest of the code
COPY . .

ENV JAVA_OPTIONS "-Xmx1500m -Dakka.test.timefactor=3"

ENTRYPOINT ["/usr/src/app/run_tests.sh"]
