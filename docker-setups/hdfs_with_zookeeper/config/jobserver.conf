spark {
  # spark.master will be passed to each job's JobContext
  master = "spark://spark-master:7077"
  submit.deployMode = "client"
  driver.supervise = false

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 1

  jobserver {
    port = 8090

    context-per-jvm = true

    dao-timeout = 10s

    kill-context-on-supervisor-down = true
    network-address-resolver =  auto
    daorootdir = "/opt/sparkjobserver/data"

    # If set to false, the binary will be cached on job start only.
    cache-on-upload = true

    startup_dao_cleanup_class = "spark.jobserver.util.ZKCleanup"
    datadao {
      rootdir = "/opt/sparkjobserver/upload"
    }

    binarydao {
        class = "spark.jobserver.io.HdfsBinaryObjectsDAO"
        # HDFS absolute path for storing binaries
        dir = "hdfs://hadoop:9000/jobserver/binaries"
    }
    metadatadao {
        class = spark.jobserver.io.zookeeper.MetaDataZookeeperDAO
    }

    zookeeperdao {
      connection-string="zookeeper:2181"
      dir = "jobserver/db"
      curator {
        retries = 3
        sleepMsBetweenRetries = 1000
        connectionTimeoutMs = 2350
        sessionTimeoutMs = 10000
      }
      autopurge = false
      autopurge_after_hours = 168
    }

    short-timeout = 10 s

    # When using chunked transfer encoding with scala Stream job results, this is the size of each chunk
    result-chunk-size = 1m
  }

  # predefined Spark contexts
  # contexts {
  #   my-low-latency-context {
  #     num-cpu-cores = 1           # Number of cores to allocate.  Required.
  #     memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
  #   }
  #   # define additional contexts here
  # }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = 2              # Number of cores to allocate.  Required.
    memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, #1G, etc.
    spark.ui.reverseProxy = false
    spark.streaming.ui.retainedBatches = 1000

    # in case spark distribution should be accessed from HDFS (as opposed to being installed on every mesos slave)
    # spark.executor.uri = "hdfs://namenode:8020/apps/spark/spark.tgz"

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]

    # Timeout for forked JVM to spin up, acquire resources and connect back to jobserver
    forked-jvm-init-timeout = 45 s

    # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,
    # such as hadoop connection settings that don't use the "spark." prefix
    passthrough {
      #es.nodes = "192.1.1.1"
    }

    launcher {
      spark.driver.memory = 1G
      spark.port.maxRetries = 100
      spark.metrics.namespace="\\${spark.app.name}"

      # Spark history server configurations
      spark.eventLog.enabled = false
      spark.eventLog.compress = true
      spark.eventLog.dir = "hdfs:///spark-history-server"
    }

    python {
      paths = [
        ${SPARK_HOME}/python,
        "/var/vcap/packages/python/lib"
      ]
      executable = "/var/vcap/packages/python/bin/python"
    }
  }

  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  # home = "/home/spark/spark"
}

akka {
  cluster {
    auto-down-unreachable-after = "off"
    downing-provider-class = "akka.downing.KeepOldestDowningProvider"
    allow-weakly-up-members = "off"
  }

  coordinated-shutdown {
    run-by-jvm-shutdown-hook = "off"
  }
}

akka.http.server {
  request-timeout = 1200 s
  idle-timeout = 1201 s
  parsing.max-content-length = 150M
  parsing.max-uri-length = 4096
}